{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DM2 NON COMPILED VERSION\n",
    "### Habiba Hmamed 22323426\n",
    "### Philippe Hinault 22006949"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import zipfile\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from pyspark.sql.functions import col, to_timestamp, udf, count, when, isnull, year, dayofweek, mean, hour, concat\n",
    "from pyspark.sql.types import StringType\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_directory = './data'\n",
    "parquet_directory = './parquet_data'\n",
    "temp_parquet_dir = './temp_parquet_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program for fetching the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_data(bucket, start_year, end_year):\n",
    "    #create the s3 client and assign credentials (UNSIGEND for public \n",
    "                     \n",
    "    client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "    years_list = [str(x) for x in list(range(start_year, end_year +1))]\n",
    "    #create a list of 'Contect' objects from the s3 bucket\n",
    "    list_files = client.list_objects(Bucket=bucket)['Contents']\n",
    "\n",
    "    if not os.path.exists('./data'):\n",
    "        os.makedirs('./data')\n",
    "\n",
    "    for key in list_files:\n",
    "        \n",
    "        if key['Key'].endswith('.zip') and key['Key'][0:4] in years_list :\n",
    "            if os.path.exists(f'./data/{key[\"Key\"]}'):\n",
    "                print (f'{key[\"Key\"]} already exists')\n",
    "            else :\n",
    "                print(f'downloading... {key[\"Key\"]}') #print file name\n",
    "                client.download_file(\n",
    "                                        Bucket=bucket, #assign bucket name\n",
    "                                        Key=key['Key'], #key is the file name\n",
    "                                        Filename=os.path.join('./data', \n",
    "                                            key['Key']) #storage file path\n",
    "                                )\n",
    "        else:\n",
    "            pass #if it's not a zip file do nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes schemas\n",
    "Two schema for the two standards in the data. Later unified into one (schema_type_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2014\n",
    "end_year = 2023\n",
    "\n",
    "tmp = \"./tmp\"\n",
    "csv_directory = './data'\n",
    "parquet_directory = './parquet_data'\n",
    "temp_parquet_dir = './temp_parquet_data'\n",
    "\n",
    "schema_type_standard_one = [\n",
    "    \"tripduration\",\n",
    "    \"start_time\", \n",
    "    \"stop_time\", \n",
    "    \"start_station_id\", \n",
    "    \"start_station_name\", \n",
    "    \"start_station_latitude\", \n",
    "    \"start_station_longitude\", \n",
    "    \"end_station_id\", \n",
    "    \"end_station_name\", \n",
    "    \"end_station_latitude\", \n",
    "    \"end_station_longitude\", \n",
    "    \"bikeid\", \n",
    "    \"user_type\", \n",
    "    \"birth_year\",\n",
    "    \"gender\",\n",
    "]\n",
    "\n",
    "schema_type_standard_two = [\n",
    "    \"ride_id\",\n",
    "    \"rideable_type\",\n",
    "    \"start_time\",\n",
    "    \"stop_time\",\n",
    "    \"start_station_name\",\n",
    "    \"start_station_id\",\n",
    "    \"end_station_name\",\n",
    "    \"end_station_id\",\n",
    "    \"start_station_latitude\",\n",
    "    \"start_station_longitude\",\n",
    "    \"end_station_latitude\",\n",
    "    \"end_station_longitude\",\n",
    "    \"user_type\",\n",
    "]\n",
    "\n",
    "schema_type_union_standard = {\n",
    "    \"ride_id\": \"string\",\n",
    "    \"rideable_type\": \"string\",\n",
    "    \"start_time\": \"timestamp\",\n",
    "    \"stop_time\": \"timestamp\",\n",
    "    \"start_station_id\": \"float\",\n",
    "    \"start_station_name\": \"string\",\n",
    "    \"start_station_latitude\": \"double\",\n",
    "    \"start_station_longitude\": \"double\",\n",
    "    \"end_station_id\": \"float\",\n",
    "    \"end_station_name\": \"string\",\n",
    "    \"end_station_latitude\": \"double\",\n",
    "    \"end_station_longitude\": \"double\",\n",
    "    \"bikeid\": \"float\",\n",
    "    \"user_type\": \"string\",\n",
    "    \"birth_year\": \"integer\",\n",
    "    \"gender\": \"string\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"CSVtoParquet\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.driver.cores\", \"4\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Library for conversion\n",
    "### to convert the csv content and turn it to the new standard, which is the two standard merged and renamed \n",
    "\n",
    "Important note: \"member\" is seemingly the new name for \"Subscriber\" in \"user_type\", it is kept unchanged because Subscriber has more informations in the old standard\n",
    "including: birth_year, gender.\n",
    "All IDs are floats, because there were IDs like \"XXX.04\" in the second standard. It is unsure if the others will be integers in the future \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the best ways to differientiate between the two schemas is by the number of columns they have\n",
    "standard_one_format = 15\n",
    "standard_two_format = 13\n",
    "\n",
    "date_format_one = 'M/d/yyyy HH:mm:ss'\n",
    "date_format_two = 'yyyy-MM-dd HH:mm:ss'\n",
    "date_format_three = 'M/d/yyyy HH:mm'\n",
    "\n",
    "\n",
    "def extract_zip_if_needed(zip_path, extract_dir):\n",
    "    if not os.path.exists(extract_dir) or not os.listdir(extract_dir):\n",
    "        print(f\"Extracting zip file: {zip_path} to directory: {extract_dir}\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_dir)\n",
    "        print(\"Extraction complete.\")\n",
    "    else:\n",
    "        print(f\"Directory {extract_dir} already exists and is not empty. Skipping extraction.\")\n",
    "\n",
    "def read_csv_to_df(csv_path, year):\n",
    "    try:\n",
    "        print(f\"Reading CSV file: {csv_path}\")\n",
    "        sdf = spark.read.option(\"header\", \"true\").csv(csv_path)\n",
    "        num_partitions = sdf.rdd.getNumPartitions()\n",
    "        print(f\"Number of partitions: {num_partitions}\")\n",
    "        storage_level = sdf.storageLevel\n",
    "        print(f\"Storage Level: {storage_level}\")\n",
    "        # Perform necessary data cleaning\n",
    "        sdf = clean_sdf_data(sdf, year)\n",
    "        sdf.show(3, truncate=False)\n",
    "        print(sdf.dtypes)\n",
    "        print(f\"CSV file {csv_path} successfully read into DataFrame.\")\n",
    "        return sdf\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {csv_path}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_all_csv_files(directory):\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # Ignore directories containing \"__MACOSX\"\n",
    "        dirs[:] = [d for d in dirs if '__MACOSX' not in d]\n",
    "        for file in files:\n",
    "            # Ignore files starting with \"._\"\n",
    "            if file.endswith('.csv') and not file.startswith('._'):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return csv_files\n",
    "\n",
    "\n",
    "def convert_from_csv(year, csv_dir, temp_parquet_dir ,cleanup=True):\n",
    "    print(f\"Looking for zip files in directory: {csv_dir} for year: {year}\")\n",
    "    zip_name = [f for f in os.listdir(csv_dir) if f.startswith(str(year))][0]\n",
    "    zip_path = os.path.join(csv_dir, zip_name)\n",
    "    \n",
    "    temp_dir = os.path.join(csv_dir, f\"temp_extract_{year}\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    print(f\"Created temporary directory: {temp_dir}\")\n",
    "    result = []\n",
    "    try:\n",
    "        extract_zip_if_needed(zip_path, temp_dir)\n",
    "        \n",
    "        csv_files = find_all_csv_files(temp_dir)\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            print(f\"Processing CSV file: {csv_file}\")\n",
    "            sdf = read_csv_to_df(csv_file, year)\n",
    "            if sdf is not None:\n",
    "                result.append(sdf)\n",
    "                print(f\"CSV file {csv_file} successfully added to result list.\")\n",
    "\n",
    "        union_all_sdf(result, year, temp_parquet_dir)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ZIP file: {zip_path}. Error: {e}\")\n",
    "    finally:\n",
    "        if cleanup:\n",
    "            print(f\"Deleting temporary directory: {temp_dir}\")\n",
    "            shutil.rmtree(temp_dir)\n",
    "            print(\"Temporary directory deleted.\")\n",
    "        else:\n",
    "            print(f\"Temporary directory left : {temp_dir}, only use this if you want to inspect the files.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def convert_gender(string_value):\n",
    "        Male = \"M\"\n",
    "        Female = \"F\"\n",
    "            \n",
    "        if string_value == \"1\":\n",
    "                return Male\n",
    "        elif string_value == \"2\":\n",
    "                return Female\n",
    "        else:\n",
    "                return \"Unknown\"\n",
    "        \n",
    "def reorder_columns(sdf, schema_order):\n",
    "    new_order = []\n",
    "\n",
    "    # Reorder the columns according to the schema order\n",
    "    for column in schema_order:\n",
    "        if column in sdf.columns:\n",
    "            new_order.append(column)\n",
    "\n",
    "    # Select the columns in the new order\n",
    "    sdf = sdf.select(*new_order)\n",
    "\n",
    "    # Cast the columns to the specified types using the schema\n",
    "    for column in new_order:\n",
    "        col_type = schema_type_union_standard.get(column)\n",
    "        if col_type:\n",
    "            sdf = sdf.withColumn(column, sdf[column].cast(col_type))\n",
    "    \n",
    "    return sdf\n",
    "\n",
    "def add_missing_columns(sdf):\n",
    "       # add missing colums from the standard schema union\n",
    "        for col in schema_type_union_standard:\n",
    "                if col not in sdf.columns:\n",
    "                        sdf = sdf.withColumn(col, lit(\"Unknown\").cast(\"string\"))\n",
    "        return sdf\n",
    "\n",
    "def convert_date_column(df, date_col):\n",
    "    # Try to convert the date column to timestamp using the first format\n",
    "    temp_col_one = to_timestamp(col(date_col), date_format_one)\n",
    "    \n",
    "    # If conversion with the first format fails, try with the third format\n",
    "    temp_col_three = when(isnull(temp_col_one), to_timestamp(col(date_col), date_format_three)).otherwise(temp_col_one)\n",
    "    \n",
    "    # If conversion with the third format fails, it's the second format\n",
    "    parsed_col = when(isnull(temp_col_three), to_timestamp(col(date_col), date_format_two)).otherwise(temp_col_three)\n",
    "    \n",
    "    # Convert to the unified format (date_format_two)\n",
    "    final_col = to_timestamp(parsed_col, date_format_two)\n",
    "    \n",
    "    # Replace the original column with the new one\n",
    "    df = df.withColumn(date_col, final_col)\n",
    "\n",
    "    return df\n",
    "\n",
    "def change_user_type(user_type):\n",
    "    if user_type == \"Customer\":\n",
    "        return \"member\"\n",
    "    elif user_type == \"Subscriber\":\n",
    "        return \"Subscriber\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def clean_sdf_first_standard (sdf, year):\n",
    "        # rename the column with the standard schema one\n",
    "        sdf = sdf.toDF(*schema_type_standard_one)\n",
    "\n",
    "        # drop the tripduration column\n",
    "        # because of redundancy and the other standard does not have it\n",
    "        # better to calculate it later if needed\n",
    "        sdf = sdf.drop(\"tripduration\")\n",
    "\n",
    "        \n",
    "        # add missing colums from the standard schema union\n",
    "        sdf = add_missing_columns(sdf)\n",
    "\n",
    "        timestamp_cols = [col for col in sdf.columns if 'date' in col or 'time' in col]\n",
    "\n",
    "        # convert timestamp columns that were str to timestamp type\n",
    "        for col_ in timestamp_cols:\n",
    "                if isinstance(sdf.schema[col_].dataType, StringType):\n",
    "                        sdf = convert_date_column(sdf, col_)\n",
    "                \n",
    "                        \n",
    "        # convert int to gender \n",
    "        # 0 is Unknown\n",
    "        # 1 is Male\n",
    "        # 2 is Female \n",
    "        convert_gender_udf = udf(convert_gender, StringType())\n",
    "\n",
    "        sdf = sdf.withColumn(\"gender\", convert_gender_udf(\"gender\"))\n",
    "\n",
    "        # change subcriber to member\n",
    "        # change customer to casual\n",
    "        change_user_type_udf = udf(change_user_type, StringType())\n",
    "        sdf = sdf.withColumn(\"user_type\", change_user_type_udf(\"user_type\"))\n",
    "        \n",
    "        # reorder the columns to match the standard schema union\n",
    "        sdf = reorder_columns(sdf, schema_type_union_standard)\n",
    "\n",
    "        return sdf\n",
    "\n",
    "def clean_sdf_second_standard (sdf):\n",
    "        # rename the column with the standard schema two\n",
    "        sdf = sdf.toDF(*schema_type_standard_two)\n",
    "\n",
    "        # add missing colums from the standard schema union\n",
    "        sdf = add_missing_columns(sdf)\n",
    "\n",
    "        # reorder the columns to match the standard schema union\n",
    "        sdf = reorder_columns(sdf, schema_type_union_standard)\n",
    "        \n",
    "        return sdf\n",
    "\n",
    "\n",
    "def clean_sdf_data (sdf, year):\n",
    "        if len (sdf.columns) == standard_one_format:\n",
    "                return clean_sdf_first_standard(sdf, year)\n",
    "        elif len(sdf.columns) == standard_two_format:\n",
    "                return clean_sdf_second_standard(sdf)\n",
    "\n",
    "        return sdf\n",
    "\n",
    "# union every df in a list of dataframes\n",
    "def union_list_sdf(list_sdf):\n",
    "    res = None\n",
    "    for sdf in list_sdf:\n",
    "        if res is None:\n",
    "            res = sdf\n",
    "        else:\n",
    "            res = res.union(sdf)\n",
    "    return res\n",
    "\n",
    "# make a parquet for every year in the range\n",
    "# all respecting a specific schema\n",
    "def union_all_sdf (sdf, year, temp_dir):\n",
    "        print(f\"Processing year: {year} for saving to Parquet.\")\n",
    "        sdf = union_list_sdf(sdf)\n",
    "        save_df_to_parquet(sdf, temp_dir, f\"year_{year}\")\n",
    "        print(f\"Year {year} successfully saved to parquet in {temp_dir} directory.\")\n",
    "        \n",
    "\n",
    "def save_df_to_parquet(df, parquet_dir, name):\n",
    "        if not os.path.exists(parquet_dir):\n",
    "                os.makedirs(parquet_dir)\n",
    "        \n",
    "        parquet_file = os.path.join(parquet_dir, name + '.parquet')\n",
    "                    \n",
    "        # Save DataFrame as Parquet\n",
    "        print(\"===============Writing to: \" + parquet_file)\n",
    "        df.withColumn(\"year\", year(\"start_time\")).write.partitionBy(\"year\").parquet(parquet_file, mode='overwrite')\n",
    "        print(\"===============Ok, just written. Now go look plz: \" + parquet_file)\n",
    "        time.sleep(10)\n",
    "\n",
    "def find_all_parquet_files(directory):\n",
    "    parquet_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                parquet_files.append(os.path.join(root, file))\n",
    "    return parquet_files\n",
    "\n",
    "import time\n",
    "def find_all_parquet_dirs(directory):\n",
    "    parquet_dirs = []\n",
    "    print(\"=======The dir: \" + directory)\n",
    "    time.sleep(20)\n",
    "    for item in os.listdir(directory):\n",
    "        print(\"Found year parquet: \" + item)\n",
    "        item_path = os.path.join(directory, item)\n",
    "        parquet_dirs.append(item_path)\n",
    "    return parquet_dirs\n",
    "\n",
    "def csv_to_parquet (start_year, end_year, csv_dir, parquet_dir, name, cleanup=True):\n",
    "    # Create a temporary directory to store intermediate Parquet files\n",
    "    temp_parquet_dir = os.path.join(csv_dir, 'temp_parquet_dir')\n",
    "\n",
    "    if not os.path.exists(temp_parquet_dir):\n",
    "        os.makedirs(temp_parquet_dir)\n",
    "\n",
    "    try:\n",
    "        # Process each year and save intermediate Parquet files\n",
    "        # union_all_sdf(start_year, end_year, csv_dir, temp_parquet_dir, cleanup)\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            convert_from_csv(year, csv_dir, temp_parquet_dir, cleanup)\n",
    "\n",
    "        # Find all intermediate Parquet files\n",
    "        parquet_files = find_all_parquet_dirs(temp_parquet_dir)\n",
    "\n",
    "        # Read all Parquet files and union them into a single DataFrame\n",
    "        if len(parquet_files) > 0:\n",
    "            res = None\n",
    "            for file in parquet_files:\n",
    "                print(f\"Reading Parquet file: {file}\")\n",
    "                df = spark.read.parquet(file)\n",
    "                if res is None:\n",
    "                    res = df\n",
    "                else:\n",
    "                    res = res.union(df)\n",
    "                    print(f\"Parquet file {file} successfully read.\")\n",
    "            # Save the final DataFrame as a single Parquet file\n",
    "            save_df_to_parquet(res, parquet_dir, name)\n",
    "            print(f\"Final Parquet file saved to {parquet_dir}/{name}.parquet\")\n",
    "        else:\n",
    "            print(\"No Parquet files found to union.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "    finally:\n",
    "        # Cleanup temporary directory if specified\n",
    "        if cleanup:\n",
    "            shutil.rmtree(temp_parquet_dir)\n",
    "            print(f\"Temporary directory {temp_parquet_dir} has been removed.\")\n",
    "        \n",
    "def get_unified_parquet (parquet_dir, name, start_year=2014, end_year=2023):\n",
    "    print(f\"Looking for directory: {parquet_dir}\")\n",
    "    if not os.path.exists(parquet_dir):\n",
    "        print(f\"Directory {parquet_dir} does not exist. \\nCreating directory: {parquet_dir}\")\n",
    "        os.makedirs(parquet_dir)\n",
    "\n",
    "    print(f\"Looking for Parquet files in directory: {parquet_dir}\")\n",
    "    fml = parquet_dir + \"/\" + name + \".parquet\"\n",
    "    print(f\"Looking for file: {fml}\")\n",
    "    if os.path.exists(fml):\n",
    "        print(f\"Found {fml} Parquet files in directory: {parquet_dir}\")\n",
    "        df = spark.read.parquet(fml)\n",
    "        return df\n",
    "        \n",
    "    print(f\"Parquet file {name} not found.\")\n",
    "    print(\"Checking and downloading the data from S3\")\n",
    "    get_s3_data('tripdata', start_year, end_year)\n",
    "    csv_to_parquet(start_year, end_year, csv_directory, parquet_dir, name)\n",
    "\n",
    "    fml = parquet_dir + \"/\" + name + \".parquet\"\n",
    "    df = spark.read.parquet(fml)\n",
    "    return df\n",
    "\n",
    "# returns unique values of a column\n",
    "def get_unique_values(df, column_name):\n",
    "    return df.select(column_name).distinct().collect()\n",
    "\n",
    "# returns repeated values of a column\n",
    "def get_repeated_rows(df, column_name):\n",
    "    # Find the values that are repeated\n",
    "    repeated_values = df.groupBy(column_name).agg(count(column_name).alias(\"count\")) \\\n",
    "                        .filter(col(\"count\") > 1) \\\n",
    "                        .select(column_name)\n",
    "    \n",
    "    # join the repeated values with the original dataframe\n",
    "    result_df = df.join(repeated_values, on=column_name, how=\"inner\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the dataframe in the unified parquet\n",
    "\n",
    "Storage Management: csv_to_parquet extract a year in a temporary directory ({year}.parquet), and save it in parquet in an another directory (temp_parquet_data). Then the extracted directory is deleted and the program move to the next year. When every year is done, they are merged into unified.parquet. Every {year}.parquet is then deleted.\n",
    "\n",
    "note: Please use the commented lines instead in case of failure of get_unified_parquet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same command if the parquet exists\n",
    "# sdf = spark.read.parquet(\"./parquet_data/unified.parquet\")\n",
    "sdf = get_unified_parquet(parquet_directory, \"unified\")\n",
    "\n",
    "\n",
    "\n",
    "# get_s3_data('tripdata', start_year, end_year)\n",
    "# csv_to_parquet(start_year, end_year, csv_directory, parquet_directory, \"unified\", True)\n",
    "# sdf = get_unified_parquet(parquet_directory, \"unified\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sdf = get_unified_parquet(tmp, \"unified2\", 2014, 2014)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the StorageLevel of the dataframe after reading the csv files ? \n",
    "According to df.storagelevel : Serialized 1x Replicated\n",
    "(same result between \"after reading csv\" and \"after reading parquet file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_level = sdf.storageLevel\n",
    "print(f\"AFTER CSV Storage Level: {storage_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the number of partition of the dataframe ? \n",
    "According to df.rdd.getNumPartitions(): 8 reading the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = sdf.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Is it possible to tune this number at loading time ? \n",
    "Yes, when you start a SparkSession (.config(\"spark.sql.shuffle.partitions\", \"8\")) \n",
    "or when a dataframe is read (read.option(\"header\", \"true\").csv(csv_path).repartition(num_partitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Why would we want to modify the number of partition when creating the parquet file ? \n",
    "To get faster, if we partition on the year, a column that is frequently used. It could be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "note: for performance issues, we mainly used a parquet that had data between 2018 and 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = get_unified_parquet(parquet_directory, \"unified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = get_unified_parquet(tmp, \"unified2\", 2019, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain function\n",
    "the optimized logical plan is shorter than the analysed logical plan in its plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_explain = get_unified_parquet(tmp, \"unified2\", 2019, 2021)\n",
    "sdf_explain = sdf_explain.withColumn(\"day_of_week\", dayofweek(sdf_explain[\"start_time\"])).select(\"day_of_week\").groupBy(\"day_of_week\").count().explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Count the number of trips for each pickup/dropoff station\n",
    "\n",
    "def cut_relevant_trip(year_n):\n",
    "    couple_station = sdf.where(year(\"start_time\") == year_n).groupBy(\"start_station_name\", \"end_station_name\").count()\n",
    "    couple_station = couple_station.withColumnRenamed(\"count\", \"number_of_trips\")\n",
    "    couple_station = couple_station.orderBy(col(\"number_of_trips\").desc())\n",
    "    return couple_station\n",
    "\n",
    "a_year_before_covid = 2019\n",
    "a_year_after_covid = 2021\n",
    "\n",
    "couple_station2019 = cut_relevant_trip(a_year_before_covid)\n",
    "couple_station2021 = cut_relevant_trip(a_year_after_covid)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,12))\n",
    "\n",
    "# Plot the top 10 most popular trips\n",
    "couple_station_pd = couple_station2019.toPandas()\n",
    "couple_station_pd = couple_station_pd.head(10)\n",
    "axs[0].bar(couple_station_pd[\"start_station_name\"], couple_station_pd[\"number_of_trips\"])\n",
    "axs[0].set_xlabel('Trip')\n",
    "axs[0].set_ylabel('Number of trips')\n",
    "axs[0].set_title('Top 10 most popular trips before covid')\n",
    "axs[0].set_xticklabels(couple_station_pd[\"start_station_name\"], rotation=90)\n",
    "couple_station_pd = couple_station2021.toPandas()\n",
    "couple_station_pd = couple_station_pd.head(10)\n",
    "axs[1].bar(couple_station_pd[\"start_station_name\"], couple_station_pd[\"number_of_trips\"])\n",
    "axs[1].set_xlabel('Trip')\n",
    "axs[1].set_ylabel('Number of trips')\n",
    "axs[1].set_title('Top 10 most popular trips after covid')\n",
    "axs[1].set_xticklabels(couple_station_pd[\"start_station_name\"], rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions from the trips:\n",
    "The couples of trips in the top 10 are not the same. But the overall, the couples of trip after are way more frequented. \n",
    "The density of cyclists in certain places could be greater.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Break down the trip distance distribution for each day of the week\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(delta_phi / 2.0)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2.0)**2\n",
    "    c = 2.0 * math.atan2(math.sqrt(a), math.sqrt(1.0 - a))\n",
    "\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "def replace_lat_long_with_distance(df):\n",
    "    haversine_udf = udf(haversine)\n",
    "    df = df \\\n",
    "        .where(col(\"start_station_latitude\").isNotNull()) \\\n",
    "        .where(col(\"start_station_longitude\").isNotNull()) \\\n",
    "        .where(col(\"end_station_latitude\").isNotNull()) \\\n",
    "        .where(col(\"end_station_latitude\").isNotNull()) \\\n",
    "        .withColumn(\"distance\", haversine_udf(\"start_station_latitude\", \"start_station_longitude\", \"end_station_latitude\", \"end_station_longitude\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_timestamp_to_day_of_week(df):\n",
    "    df = df.withColumn(\"day_of_week\", dayofweek(df[\"start_time\"]))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df = replace_lat_long_with_distance(sdf)\n",
    "df = convert_timestamp_to_day_of_week(df)\n",
    "\n",
    "\n",
    "distance_pd2019 = df.where(year(\"start_time\") == 2019).select(\"day_of_week\", \"distance\").groupBy(\"day_of_week\").agg(mean(\"distance\").alias(\"distance\")).toPandas()\n",
    "distance_pd2021 = df.where(year(\"start_time\") == 2021).select(\"day_of_week\", \"distance\").groupBy(\"day_of_week\").agg(mean(\"distance\").alias(\"distance\")).toPandas()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,6))\n",
    "axs[0].bar(distance_pd2019[\"day_of_week\"], distance_pd2019[\"distance\"])\n",
    "axs[0].set_xlabel('Day of week')\n",
    "axs[0].set_ylabel('Distance')\n",
    "axs[0].set_title('Distance by day of week')\n",
    "axs[1].bar(distance_pd2021[\"day_of_week\"], distance_pd2021[\"distance\"])\n",
    "axs[1].set_xlabel('Day of week')\n",
    "axs[1].set_ylabel('Distance')\n",
    "axs[1].set_title('Distance by day of week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion from trip distance distribution for each day of the week\n",
    "the distance is negligeable between each day of the week, with a slight peek after the covid on sunday and saturday. (if we consider the first day is sunday)\n",
    "but overall, after the covid the distance increased. The distance is done by calculating the flight time (which is wrong, but we should focus on the proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip distance distribution for gender\n",
    "# gender is only available for the first schema, where there is \"Subscriber\"\n",
    "def cut_to_available_Subscriber (df):\n",
    "    df = df.where(col(\"user_type\") == \"Subscriber\")\n",
    "    return df\n",
    "\n",
    "df = replace_lat_long_with_distance(sdf)\n",
    "df = cut_to_available_Subscriber(df)\n",
    "df = df.select(\"gender\", \"distance\")\n",
    "\n",
    "gender_pd2019 = df.where(year(\"start_time\") == 2019).groupBy(\"gender\").agg(mean(\"distance\").alias(\"distance\")).toPandas()\n",
    "gender_pd2021 = df.where(year(\"start_time\") == 2021).groupBy(\"gender\").agg(mean(\"distance\").alias(\"distance\")).toPandas()\n",
    "\n",
    "figs, axs = plt.subplots(1, 2, figsize=(10,3))\n",
    "axs[0].bar(gender_pd2019[\"gender\"], gender_pd2019[\"distance\"])\n",
    "axs[0].set_xlabel('Gender')\n",
    "axs[0].set_ylabel('Distance')\n",
    "axs[0].set_title('Distance by gender')\n",
    "axs[1].bar(gender_pd2021[\"gender\"], gender_pd2019[\"distance\"])\n",
    "axs[1].set_xlabel('Gender')\n",
    "axs[1].set_ylabel('Distance')\n",
    "axs[1].set_title('Distance by gender')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions from trip distance for gender\n",
    "Women tend do to more distance than men and people that did not specify their gender. This plot was made with the subscriber, where they can specify their gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip distance distribution for age ranges\n",
    "# age is only available for the first schema, where there is \"Subscriber\"\n",
    "def cut_to_available_age (df, start_age, end_age):\n",
    "    df = df.where(col(\"birth_year\").isNotNull())\n",
    "    df = df.withColumn(\"age\", year(\"start_time\") - col(\"birth_year\"))\n",
    "    df = df.where(col(\"age\") >= start_age).where(col(\"age\") <= end_age)\n",
    "    return df\n",
    "\n",
    "def cut_to_over_age (df, start_age):\n",
    "    df = df.where(col(\"birth_year\").isNotNull())\n",
    "    df = df.withColumn(\"age\", year(\"start_time\") - col(\"birth_year\"))\n",
    "    df = df.where(col(\"age\") >= start_age)\n",
    "    return df\n",
    "\n",
    "range_one = (15, 24)\n",
    "range_two = (25, 44)\n",
    "range_three = (45, 54)\n",
    "range_four = (55, 64)\n",
    "range_five = (65, 200)\n",
    "\n",
    "dff = replace_lat_long_with_distance(sdf)\n",
    "\n",
    "def set_axs (axs, range, x, y, year, df):\n",
    "    axs[x, y].bar(df[\"age\"], df[\"distance\"])\n",
    "    axs[x, y].set_xlabel('Age')\n",
    "    axs[x, y].set_ylabel('Distance')\n",
    "    axs[x, y].set_title(f'Distance by age range {range[0]}-{range[1]} in {year}')\n",
    "    return axs\n",
    "\n",
    "def df_range (df, range, y, axs):\n",
    "    df = cut_to_available_age(dff, range[0], range[1])\n",
    "    df_2019 = df.where(year(\"start_time\") == 2019).groupBy(\"age\").agg(mean(\"distance\").alias(\"distance\")).toPandas()\n",
    "    df_2021 = df.where(year(\"start_time\") == 2021).groupBy(\"age\").agg(mean(\"distance\").alias(\"distance\")).toPandas()\n",
    "    axs = set_axs(axs, range, 0, y, 2019, df_2019)\n",
    "    axs = set_axs(axs, range, 1, y, 2021, df_2021)\n",
    "    return axs\n",
    "\n",
    "figs, axs = plt.subplots(2,5, figsize=(18,10))\n",
    "\n",
    "axs = df_range(dff, range_one, 0, axs)\n",
    "axs = df_range(dff, range_two, 1, axs)\n",
    "axs = df_range(dff, range_three, 2, axs)\n",
    "axs = df_range(dff, range_four, 3, axs)\n",
    "axs = df_range(dff, range_five, 4, axs)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions for distance distribution by range of age\n",
    "Overall, everywhere there is an increase in distance for every range of age, but we see a good increase between 15 and 45 and a huge increase for the 65+. (Wrong value because it is unlikely that a 150 year old is doing 4 times as much distance as a 25 year old person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip distance distribution for different types of bikes\n",
    "def cut_to_available_bike (df):\n",
    "    df = df.where(col(\"rideable_type\") != \"Unknown\")\n",
    "    return df\n",
    "\n",
    "df_bike = replace_lat_long_with_distance(sdf)\n",
    "df_bike = cut_to_available_bike(df_bike)\n",
    "\n",
    "bike_pd2019 = df_bike.where(year(\"start_time\") == 2019).groupBy(\"rideable_type\").agg(mean(\"distance\").alias(\"distance\")).toPandas()\n",
    "bike_pd2021 = df_bike.where(year(\"start_time\") == 2021).groupBy(\"rideable_type\").agg(mean(\"distance\").alias(\"distance\")).toPandas()\n",
    "\n",
    "figs, axs = plt.subplots()\n",
    "axs.bar(bike_pd2021[\"rideable_type\"], bike_pd2021[\"distance\"])\n",
    "axs.set_xlabel('Bike type')\n",
    "axs.set_ylabel('Distance')\n",
    "axs.set_title('Distance by bike type in 2021')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion to Distance by bike type\n",
    "obviously, more distance with an electric one\n",
    "and there is no information on bike types in 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph: time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of docks indexed by day of the week and hour of the day\n",
    "\n",
    "def add_hour_pickup (df):\n",
    "    df = df.withColumn(\"hour_pickup\", hour(df[\"start_time\"]))\n",
    "    return df\n",
    "\n",
    "def add_hour_docks (df):\n",
    "    df = df.withColumn(\"hour_docks\", hour(df[\"stop_time\"]))\n",
    "    return df\n",
    "\n",
    "def add_day_of_week (df):\n",
    "    df = df.withColumn(\"day_of_week_pickup\", dayofweek(df[\"start_time\"]))\n",
    "    df = df.withColumn(\"day_of_week_docks\", dayofweek(df[\"stop_time\"]))\n",
    "    return df\n",
    "\n",
    "def select_docks (df):\n",
    "    df = df.select(\"hour_pickup\", \"hour_docks\" ,\"day_of_week_pickup\", \"day_of_week_docks\")\n",
    "    return df\n",
    "\n",
    "def cut_days (df, day):\n",
    "    df = df.where(col(\"day_of_week\") == day)\n",
    "    return df\n",
    "\n",
    "#add a column with the count of time an hour is repeated for a day\n",
    "def count_hour_day (df):\n",
    "    return df\n",
    "\n",
    "\n",
    "ssdf = add_hour_docks(sdf)\n",
    "ssdf = add_hour_pickup(ssdf)\n",
    "ssdf = add_day_of_week(ssdf)\n",
    "ssdf = select_docks(ssdf)\n",
    "\n",
    "\n",
    "ssdf = ssdf.where(col(\"start_time\").isNotNull()).where(col(\"stop_time\").isNotNull())\n",
    "ssdf_pick = ssdf.select(concat(\"day_of_week_pickup\", lit(\" - \"), \"hour_pickup\").alias(\"time\")).groupBy(\"time\").count().orderBy(\"time\")\n",
    "ssdf_docks = ssdf.select(concat(\"day_of_week_docks\", lit(\" - \"), \"hour_docks\").alias(\"time\")).groupBy(\"time\").count().orderBy(\"time\")\n",
    "\n",
    "pd_sdfp = ssdf_pick.toPandas()\n",
    "pd_sdfd = ssdf_docks.toPandas()\n",
    "figs, axs = plt.subplots(2, 1, figsize=(25,7))\n",
    "axs[0].plot(pd_sdfp[\"time\"], pd_sdfp[\"count\"])\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[0].set_title('Hour of pickup by day of week')\n",
    "axs[0].set_xticklabels(pd_sdfp[\"time\"], rotation=90)\n",
    "axs[1].plot(pd_sdfd[\"time\"], pd_sdfd[\"count\"])\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Count')\n",
    "axs[1].set_title('Hour of pickup by day of week')\n",
    "axs[1].set_xticklabels(pd_sdfd[\"time\"], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions on number of pickup/docks by day of week and time of day\n",
    "We have a peak on the weekend on mid day. \n",
    "And a peak around 17 during the week, probably for going home after work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average trip distance by day of the week and hour of the day\n",
    "\n",
    "dist_df = replace_lat_long_with_distance(sdf)\n",
    "dist_df = dist_df.withColumn(\"day_of_week\", dayofweek(df[\"start_time\"]))\n",
    "dist_df = dist_df.withColumn(\"hour\", hour(df[\"start_time\"]))\n",
    "dist_df = dist_df.withColumn(\"time\", concat(\"day_of_week\", lit(\" - \"), \"hour\"))\n",
    "dist_df = dist_df.where(col(\"time\").isNotNull())\n",
    "dist_df = dist_df.select(\"time\", \"distance\")\n",
    "\n",
    "dist_df = dist_df.groupBy(\"time\").agg(mean(\"distance\").alias(\"distance\"))\n",
    "\n",
    "dist_pd = dist_df.toPandas()\n",
    "figs, axs = plt.subplots(figsize=(25,7))\n",
    "\n",
    "axs.plot(dist_pd[\"time\"], dist_pd[\"distance\"])\n",
    "\n",
    "axs.set_xlabel('Time')\n",
    "axs.set_ylabel('Distance')\n",
    "axs.set_title('Average trip distance by day of week and hour of the day')\n",
    "axs.set_xticklabels(dist_pd[\"time\"], rotation=90)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on average distance indexed by day of the week and time of the day \n",
    "We remind that is flight distance, not real distance.\n",
    "It is pretty inconsistent with no real pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average trip duration by day of the week and hour of the day*\n",
    "\n",
    "duration_df = sdf.withColumn(\"day_of_week\", dayofweek(df[\"start_time\"]))\n",
    "duration_df = duration_df.withColumn(\"hour\", hour(df[\"start_time\"]))\n",
    "duration_df = duration_df.withColumn(\"time\", concat(\"day_of_week\", lit(\" - \"), \"hour\"))\n",
    "duration_df = duration_df.where(col(\"time\").isNotNull())\n",
    "duration_df = duration_df.withColumn(\"duration\", col(\"stop_time\").cast(\"long\") - col(\"start_time\").cast(\"long\"))\n",
    "duration_df = duration_df.select(\"time\", \"duration\", \"day_of_week\")\n",
    "\n",
    "duration_df = duration_df.groupBy(\"time\").agg(mean(\"duration\").alias(\"duration\"))\n",
    "\n",
    "duration_pd = duration_df.toPandas()\n",
    "\n",
    "figs, axs = plt.subplots(figsize=(25,7))\n",
    "axs.plot(duration_pd[\"time\"], duration_pd[\"duration\"])\n",
    "axs.set_xlabel('Time')\n",
    "axs.set_ylabel('Duration')\n",
    "axs.set_title('Average trip duration by day of week and hour of the day')\n",
    "axs.set_xticklabels(duration_pd[\"time\"], rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on Average duration index by day of the week and time of the day\n",
    "We see peaks throughout the week, but very late in the day or early in the morning.\n",
    "People are taking the bicycle when there is less trains or buses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
